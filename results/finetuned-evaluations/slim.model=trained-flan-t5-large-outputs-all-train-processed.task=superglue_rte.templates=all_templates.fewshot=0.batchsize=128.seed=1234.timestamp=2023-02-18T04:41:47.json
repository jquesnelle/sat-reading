{
  "results": [
    {
      "task_name": "superglue_rte",
      "prompt_name": "GPT-3 style",
      "acc": 0.8447653429602888,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.021797558224296024
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "GPT-3 style",
      "acc_norm": 0.851985559566787,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.021375368956596676
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "MNLI crowdsource",
      "acc": 0.8772563176895307,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.019751873032017933
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "MNLI crowdsource",
      "acc_norm": 0.8592057761732852,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.020935651096280112
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "based on the previous passage",
      "acc": 0.8953068592057761,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.018428523011352407
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "based on the previous passage",
      "acc_norm": 0.8880866425992779,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.018976411543230338
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "can we infer",
      "acc": 0.8880866425992779,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.018976411543230345
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "can we infer",
      "acc_norm": 0.8844765342960289,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.01924082640212332
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "does it follow that",
      "acc": 0.8844765342960289,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.01924082640212332
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "does it follow that",
      "acc_norm": 0.8772563176895307,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.019751873032017937
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "does this imply",
      "acc": 0.9061371841155235,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.017554530741679085
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "does this imply",
      "acc_norm": 0.8953068592057761,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.018428523011352407
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "guaranteed true",
      "acc": 0.8953068592057761,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.018428523011352407
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "guaranteed true",
      "acc_norm": 0.8736462093862816,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.019998959231186532
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "justified in saying",
      "acc": 0.9025270758122743,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.017853261915048366
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "justified in saying",
      "acc_norm": 0.8880866425992779,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.018976411543230327
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "must be true",
      "acc": 0.8772563176895307,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.019751873032017926
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "must be true",
      "acc_norm": 0.8628158844765343,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.020708871757340905
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "should assume",
      "acc": 0.8916967509025271,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_stderr": 0.018705735706082492
    },
    {
      "task_name": "superglue_rte",
      "prompt_name": "should assume",
      "acc_norm": 0.8844765342960289,
      "dataset_path": "super_glue",
      "dataset_name": "rte",
      "subset": null,
      "acc_norm_stderr": 0.019240826402123323
    }
  ],
  "config": {
    "model": "hf-seq2seq",
    "model_args": "use_accelerate=True,pretrained=trained/flan-t5-large-outputs-all-train-processed",
    "task_args": "",
    "num_fewshot": 0,
    "batch_size": 128,
    "device": null,
    "use_cache": false,
    "limit": null,
    "bootstrap_iters": 100000,
    "seed": 1234
  }
}